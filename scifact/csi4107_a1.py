"""
CSI4107 Assignment 1 - Vector Space Model IR System

This script implements a simple Information Retrieval system that:
  1. Preprocesses the corpus (tokenizes, removes stopwords, normalizes).
  2. Builds an inverted index to store term frequencies per document.
  3. Calculates IDF for each term in the vocabulary.
  4. Processes queries and computes TF-IDF based cosine similarity scores.
  5. Outputs ranked results (top-1000 by default) in TREC format to 'results.txt'.
  6. Prints a random sample of 100 tokens from the vocabulary (for documentation).

File Requirements:
  - corpus.jsonl (the SciFact corpus)
  - queries.jsonl (list of queries)
  - results.txt   (output file, automatically generated by the script)

Usage:
  python csi4107_a1.py
"""

import jsonlines
import os
import re
import math
from collections import defaultdict
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

def preprocess(text):
    """
    Step 1: Preprocessing
    - Tokenizes the text to extract words.
    - Removes standard English stopwords.
    - Converts everything to lowercase for uniformity.

    :param text: Raw string of text (title + body).
    :return: List of filtered tokens (strings).
    """
    # Simple regex-based tokenization, capturing alphanumeric word characters
    tokens = re.findall(r"\b\w+\b", text.lower())

    # Stopword removal using sklearn's set of English stopwords
    filtered_tokens = [token for token in tokens if token not in ENGLISH_STOP_WORDS]

    return filtered_tokens


def build_inverted_index(corpus_path):
    """
    Step 2: Build the Inverted Index
    - Reads the corpus (JSONL) line by line.
    - For each document, preprocesses text to extract tokens.
    - Calculates term frequency (TF) for each token.
    - Updates the inverted index with (doc_id, count).
    - Also stores document lengths for normalization steps later.

    :param corpus_path: Path to the corpus.jsonl file.
    :return: (inverted_index, document_lengths)
             inverted_index: dict {token -> [(doc_id, tf), ...]}
             document_lengths: dict {doc_id -> total_token_count}
    """
    inverted_index = defaultdict(list)
    document_lengths = {}  # To store the length of each document (for normalization)

    with jsonlines.open(corpus_path) as reader:
        for doc in reader:
            doc_id = doc["_id"]
            title = doc.get("title", "")
            text = doc.get("text", "")

            # Combine title and text for indexing
            content = title + " " + text
            tokens = preprocess(content)

            # Compute raw term frequencies for this document
            tf = defaultdict(int)
            for token in tokens:
                tf[token] += 1

            # Update the inverted index
            for token, count in tf.items():
                inverted_index[token].append((doc_id, count))

            # Store document length (number of tokens after preprocessing)
            document_lengths[doc_id] = len(tokens)

    return inverted_index, document_lengths


def calculate_idf(inverted_index, total_docs):
    """
    Step 3: Calculate IDF Values
    - IDF(term) = ln(total_docs / doc_freq(term))

    :param inverted_index: dict {token -> [(doc_id, tf), ...]}
    :param total_docs:     Total number of documents in the corpus
    :return:               dict {term -> idf_value}
    """
    idf = {}
    for term, postings in inverted_index.items():
        # len(postings) is the number of docs containing this term
        idf[term] = math.log(total_docs / len(postings))
    return idf


def rank_documents(query, inverted_index, idf, document_lengths):
    """
    Step 4: Query Processing and Ranking
    - Preprocess the query the same way as the corpus.
    - Compute a TF-IDF vector for the query.
    - For each term in the query, iterate over all documents that contain it.
    - Accumulate a partial score = (query_weight) * (document_weight).
    - Document weight = tf * idf, query weight = (query_tf * idf).
    - Normalize by document length to approximate cosine similarity.
    - Sort documents by descending score and return the ranked list.

    :param query:            String of the query text.
    :param inverted_index:   dict {token -> [(doc_id, tf), ...]}
    :param idf:              dict {term -> idf_value}
    :param document_lengths: dict {doc_id -> total_token_count}
    :return:                 List of (doc_id, score), sorted descending by score.
    """
    # Preprocess the query: tokenize, remove stopwords, etc.
    query_tokens = preprocess(query)

    # Compute TF for the query terms
    query_tf = defaultdict(int)
    for token in query_tokens:
        query_tf[token] += 1

    # Compute query TF-IDF
    query_vector = {}
    for token, count in query_tf.items():
        if token in idf:
            query_vector[token] = count * idf[token]

    # Calculate scores for documents
    scores = defaultdict(float)
    for token, weight in query_vector.items():
        if token in inverted_index:
            # For each doc containing this token
            for doc_id, tf in inverted_index[token]:
                # doc_weight = tf * idf[token]
                scores[doc_id] += weight * (tf * idf[token])

    # Normalize by document length to approximate cosine similarity
    for doc_id in scores:
        scores[doc_id] /= max(document_lengths[doc_id], 1e-6)

    # Sort by descending score
    ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return ranked_docs


def main():
    """
    Main Script
    -----------
    1. Locates the corpus.jsonl and queries.jsonl (same directory).
    2. Builds the inverted index and calculates IDF.
    3. Prints a random sample of 100 tokens from the vocabulary.
    4. Processes each query and ranks the top 1000 documents (can adjust to top 100).
    5. Writes results in TREC format to results.txt.
    """
    # File paths
    current_dir = os.path.dirname(os.path.abspath(__file__))
    corpus_path = os.path.join(current_dir, "corpus.jsonl")
    queries_path = os.path.join(current_dir, "queries.jsonl")
    results_path = os.path.join(current_dir, "results.txt")

    # Step 1 & 2: Build the inverted index
    print("Building inverted index...")
    inverted_index, document_lengths = build_inverted_index(corpus_path)
    total_docs = len(document_lengths)

    # Print sample of 100 tokens in the vocabulary
    vocabulary = list(inverted_index.keys())
    import random
    sample_size = min(100, len(vocabulary))
    sample_tokens = random.sample(vocabulary, sample_size)
    print("\n========== SAMPLE OF 100 VOCABULARY TOKENS ==========")
    for i, token in enumerate(sample_tokens, start=1):
        print(f"{i}. {token}")
    print("====================================================\n")

    # Step 3: Calculate IDF values
    print("Calculating IDF values...")
    idf = calculate_idf(inverted_index, total_docs)

    # Step 4: Process queries and rank documents
    print("Processing queries...")
    with jsonlines.open(queries_path) as reader:
        queries = list(reader)

    # Open results file in write mode
    with open(results_path, "w") as results_file:
        for query in queries:
            query_id = query["_id"]
            query_text = query.get("text", "")

            # Skip empty queries if any
            if not query_text:
                continue

            ranked_docs = rank_documents(query_text, inverted_index, idf, document_lengths)

            # Write top-1000 results (you can reduce to top-100 if needed)
            for rank, (doc_id, score) in enumerate(ranked_docs[:1000], start=1):
                # TREC format: query_id Q0 doc_id rank score run_name
                results_file.write(f"{query_id} Q0 {doc_id} {rank} {score:.4f} run_name\n")

    print(f"Results saved to {results_path}")


if __name__ == "__main__":
    main()
